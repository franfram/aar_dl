{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqRs1vydLmQGpJTxlgUiqB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franfram/AAR-DL/blob/main/aar_dl_CNN_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF tutorials:\n",
        "\n",
        "https://www.tensorflow.org/tutorials/load_data/images\n",
        "https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\n",
        "https://www.tensorflow.org/tutorials/images/classification\n",
        "https://www.tensorflow.org/guide/data_performance\n",
        "https://www.tensorflow.org/guide/keras\n",
        "https://www.tensorflow.org/tutorials/keras/classification\n",
        "https://www.tensorflow.org/tutorials/"
      ],
      "metadata": {
        "id": "3v1nTp1KTXag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following comes from\n",
        "https://www.tensorflow.org/tutorials/load_data/images"
      ],
      "metadata": {
        "id": "tabFDCA7eDCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will build a Convolutional Neural Network (CNN) from scratch using TensorFlow and Keras (TFs high level API).\n",
        "\n"
      ],
      "metadata": {
        "id": "Wadu38omXhY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZZBCrLCeXqUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup (Needs cleaning)\n",
        "!pip install fastai\n",
        "!pip install --upgrade Pillow\n",
        "from fastai import *\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import PIL.Image\n",
        "from IPython.display import display\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import math\n",
        "import random\n",
        "\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s2_AL4StYGSr",
        "outputId": "105ec84b-941d-4203-adb0-da31224cdcdd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastai in /usr/local/lib/python3.10/dist-packages (2.7.12)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastai) (23.1)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai) (1.5.29)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from fastai) (0.15.2+cu118)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fastai) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fastai) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fastai) (2.31.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from fastai) (6.0.1)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai) (1.0.3)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.10/dist-packages (from fastai) (9.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fastai) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fastai) (1.10.1)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai) (3.6.1)\n",
            "Requirement already satisfied: torch<2.1,>=1.7 in /usr/local/lib/python3.10/dist-packages (from fastai) (2.0.1+cu118)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.23.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.7->fastai) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.7->fastai) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.7->fastai) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.7->fastai) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.7->fastai) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=1.7->fastai) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=1.7->fastai) (16.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fastai) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastai) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastai) (3.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4->fastai) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4->fastai) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.1,>=1.7->fastai) (1.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Collecting Pillow\n",
            "  Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "Successfully installed Pillow-10.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get the data\n",
        "def fetch_data():\n",
        "  \"\"\"Fetches required data from github repo and sets up the Files in google colab \"\"\"\n",
        "\n",
        "  if not os.path.exists('AAR-DL'):\n",
        "    !git clone https://github.com/franfram/AAR-DL\n",
        "\n",
        "  !mv AAR-DL/behaviour-images behaviour-images\n",
        "  !rm -rf AAR-DL\n",
        "  !rm -rf sample_data\n",
        "\n",
        "\n",
        "fetch_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1tu76sdXiVb",
        "outputId": "770276a0-7dbb-4724-b077-018e3ae157b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AAR-DL'...\n",
            "remote: Enumerating objects: 2453, done.\u001b[K\n",
            "remote: Counting objects: 100% (2453/2453), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2325/2325), done.\u001b[K\n",
            "remote: Total 2453 (delta 202), reused 2335 (delta 89), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2453/2453), 19.89 MiB | 12.51 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_folder = Path('behaviour-images')\n",
        "\n",
        "\n",
        "holis = [file for subfolder in top_folder.iterdir() for file in subfolder.glob('*.png')]\n",
        "\n",
        "\n",
        "holis == png_paths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFFjz2Ad7F2Z",
        "outputId": "0058acce-955f-41bf-9dc6-2589bd20d7a1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title get image paths\n",
        "\n",
        "def get_image_paths(\n",
        "    image_dir: str = 'behaviour-images' # name of parent folder with all image data\n",
        ") -> tuple[list[pathlib.PosixPath], list[str], pathlib.PosixPath]:\n",
        "  \"\"\"Crawls through the folder containing the data and returns all the image paths, both in PosixPath and Str\"\"\"\n",
        "\n",
        "  data_dir = pathlib.Path(image_dir)\n",
        "  posix_paths = [file for subfolder in data_dir.iterdir() for file in subfolder.glob('*.png')]\n",
        "  str_paths = [str(path) for path in posix_paths]\n",
        "\n",
        "  return posix_paths, str_paths, data_dir\n",
        "\n",
        "\n",
        "posix_paths, str_paths, data_dir = get_image_paths(image_dir = 'behaviour-images')\n",
        "\n",
        "posix_paths, str_paths, data_dir"
      ],
      "metadata": {
        "id": "-QOA9kqSV_7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Display some of the image data\n",
        "\n",
        "def display_random_images_with_titles(file_paths: list, N: int):\n",
        "    \"\"\"Open and display N randomly selected images from the given paths, with their subfolder names as the titles.\"\"\"\n",
        "\n",
        "    # Randomly select N file paths\n",
        "    sampled_paths = random.sample(file_paths, N)\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    columns = math.ceil(math.sqrt(N))\n",
        "    rows = math.ceil(N / columns)\n",
        "\n",
        "    # Create a new figure\n",
        "    fig, axes = plt.subplots(rows, columns, figsize=(15, 15))  # Adjust figsize for your needs\n",
        "\n",
        "    # If only one row or column, ensure axes is 2D array\n",
        "    if rows == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    if columns == 1:\n",
        "        axes = axes.reshape(-1, 1)\n",
        "\n",
        "    for ax, file_path in zip(axes.ravel(), sampled_paths):\n",
        "        # Open the image using PIL\n",
        "        image = Image.open(file_path)\n",
        "\n",
        "        # Extract the subfolder name from the path\n",
        "        subfolder_name = Path(file_path).parts[-2]\n",
        "\n",
        "        # Display the image with the subfolder name as the title\n",
        "        ax.imshow(image)\n",
        "        ax.set_title(subfolder_name)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Hide any remaining axes\n",
        "    for ax in axes.ravel()[N:]:\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "display_random_images_with_titles(str_paths, N = 8)\n"
      ],
      "metadata": {
        "id": "bovZSLIQfAkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load data using a Keras utility\n",
        "\n",
        "# Create dataset\n",
        "\n",
        "## define some parameters for the loader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "img_width, img_height = Image.open(str_paths[0]).size # all the images are the same size\n",
        "\n",
        "\n",
        "## create the training dataset\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  'behaviour-images',\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EoiEqQDymGmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create the validation dataset\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  'behaviour-images',\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size\n",
        ")\n"
      ],
      "metadata": {
        "id": "dV5ORE2oo652"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find the `class_names` attribute on these datasets in the following way:"
      ],
      "metadata": {
        "id": "VMnjHgMfpD85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "f8B1NXsso20g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also iterate manually over the dataset andd retrieve batches of images:\n"
      ],
      "metadata": {
        "id": "_4ibDkkbrZqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "6HwqQh9bpyvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `image_batch` is a tensor of the shape `(32, 3, 200, 3)`. This is a batch of 32 images of shape `3x200x3` (the last dimension refers to the channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nPJO5ZSGrp4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want, we can convert these tensors to `numpy.ndarray`s with `.numpy()`"
      ],
      "metadata": {
        "id": "Iy2m_o4010a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(image_batch)"
      ],
      "metadata": {
        "id": "5wFaUEPKpySB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch.shape, image_batch[0].shape, image_batch[0][0].shape, image_batch[0, :, :, 1].shape"
      ],
      "metadata": {
        "id": "fpC7q8rx6oC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch[0, :, :, ] # WHY DO i HAVE 3 CHANNELS IF IS A GREY IMAGE? Answer: the 3 channels are the same, this is probably something default that TF DOES"
      ],
      "metadata": {
        "id": "mG5e2sgI7ThV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "probing that all 3 channels are the same"
      ],
      "metadata": {
        "id": "h1tLCjwx8SnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a, b, c = image_batch[0, :, :,0], image_batch[0, :, :, 1], image_batch[0, :, :, 2]"
      ],
      "metadata": {
        "id": "k-E4MEVu7qqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a, b\n",
        "np.array_equal(a, b), np.array_equal(a, c)"
      ],
      "metadata": {
        "id": "216phSD-7yf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ToDO: check if you have to do something different when you have 1 channel. (probably is just a 1D CNN?)"
      ],
      "metadata": {
        "id": "sPD4MOgh85s9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standarize the data\n",
        "\n",
        "The RGB channel values are in the [0, 255] range. This is not ideal for a neural net, in general you should seed to make the input values small. We will standarize the values to be in the [0, 1] range.\n",
        "We will do it here just to show how it works, but then we will include the normalization layer inside the model definition to simplify deployment.\n",
        "\n"
      ],
      "metadata": {
        "id": "hI_phZ9f6pJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "image_batch, labels_batch = next(iter(normalized_ds))\n",
        "\n",
        "\n",
        "image_batch, labels_batch # note that now image_batch is in the [0, 1] range but labels_batch remain unchanged\n",
        "#e.g.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvmbXPC16hQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nOtLTpHqOrcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure the dataset for performance\n",
        "Let's make sure to use buffered prefetching so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data:\n",
        "\n",
        "  - Dataset.cache keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
        "  \n",
        "  - Dataset.prefetch overlaps data preprocessing and model execution while training.\n",
        "\n",
        "Interested readers can learn more about both methods, as well as how to cache data to disk in the Prefetching section of the Better performance with the tf.data API guide. (https://www.tensorflow.org/guide/data_performance)\n",
        "\n"
      ],
      "metadata": {
        "id": "4HN506QiNFFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "train_ds, val_ds"
      ],
      "metadata": {
        "id": "MvcS7bwOMvKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "8X2fQkooPW5K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3AWlPC_YcZOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(set(np.array(labels_batch)))\n",
        "\n",
        "\n",
        "# define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(padding='same'),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(padding='same'),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(padding='same'),\n",
        "    # tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "Wl8Yisk9PuOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "AUn7IsA_RnEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=3\n",
        ")"
      ],
      "metadata": {
        "id": "pPYDdS7lSgHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO FIX LATER"
      ],
      "metadata": {
        "id": "f1XUmhVzeGu1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YsP1Jkh8eITw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_H08oqVeJH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rV-Hok8xeJEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VyIcvT6leI_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jMyy9Q26eIwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r7Z_F9Sne6wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following comes from: https://www.tensorflow.org/tutorials/keras/classification  and   https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
        "\n"
      ],
      "metadata": {
        "id": "wLDFN8Ooe5RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n"
      ],
      "metadata": {
        "id": "-r1xo4TcfFuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get the data\n",
        "if not os.path.exists('AAR-DL'):\n",
        "  !git clone https://github.com/franfram/AAR-DL\n",
        "\n",
        "!mv AAR-DL/behaviour-images behaviour-images\n",
        "!rm -rf AAR-DL\n",
        "!rm -rf sample_data\n"
      ],
      "metadata": {
        "id": "54jK1BJefFsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_dir = pathlib.Path('behaviour-images')\n",
        "\n",
        "png_paths = [file for subfolder in data_dir.iterdir() for file in subfolder.glob('*.png')]\n",
        "image_count = len(png_paths)\n",
        "image_count, png_paths\n",
        "\n"
      ],
      "metadata": {
        "id": "DISXhqu4fFpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jnGToPtefFms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9WRpYI7fFkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w-YPuhaVfFhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following comes from:\n",
        "https://www.tensorflow.org/tutorials/images/classification"
      ],
      "metadata": {
        "id": "Hw0DQSQueJzM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tf_YP9MNePsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "unlZxEi2cvXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONTINUE WITH https://www.tensorflow.org/tutorials/images/cnn"
      ],
      "metadata": {
        "id": "wk_Qy2lncwY0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSDGX2ancwFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k0rc4bDJcwC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ds5IYUnVcwAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1SHGavzhcv8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oHxcxX9McvwN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}