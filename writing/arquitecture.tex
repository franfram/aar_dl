Things to ask to Colin: 
- a kernel_size of 3 is common, but isn't this too small considering we have 40Hz data and 6.4s per label?
- Number of each parameter in general? e.g., CNN filters?
- number of heads in the Transformer MultiSelfAttention layer? More heads --> more capacity to learn temporal patterns and representations but also more computational cost. Should I just try different things or maybe do hyperparam sweep?

Things to tweak depending on performance and overfitting/underfitting: 
- Amount of Conv1D layers: too many may lead to overfitting, to few may underfit
- Amount of filters: too many might overfit, too few might underfit
- 



Certainly! Let's delve into the details of each component:

    Conv1D:
        Explanation: Conv1D, or one-dimensional convolution, is a type of layer designed to operate over sequential data. By sliding a filter (or kernel) over the sequence, this layer can detect local patterns within a specified window size (kernel_size). The primary objective of convolution layers is to identify and extract features from the data. In the context of animal movement data, Conv1D layers can recognize patterns within sequences of the accelerometer and magnetometer readings, helping the model differentiate between behaviors like walking, eating, and resting.

    MaxPooling1D:
        Explanation: MaxPooling1D layers reduce the spatial dimensions of the input data by selecting the maximum value over a specified window (pool_size) and then moving the window according to the specified stride (default is the pool size). This has the dual benefit of providing a form of translation invariance (since it's focusing on the most prominent feature in the window) and reducing the computational demands for the deeper layers. By successively halving data's spatial dimension, the model can focus on broader and more abstract features while also accelerating the computation.

    Flatten:
        Explanation: The Flatten layer is used to convert the multi-dimensional output of the previous layers into a one-dimensional vector. This transformation is necessary because after the convolutional and pooling operations, you often want to input the data into fully connected layers (Dense layers) for further processing or classification. Dense layers expect a one-dimensional input, so the Flatten layer serves as a bridge between the multi-dimensional spatial data and the Dense layers.

    Dense:
        Explanation: Dense layers, also known as fully connected layers, consist of neurons that connect to all activations in the previous layer. They are typically placed after convolutional and pooling layers to produce the final classification results. In your case, the last Dense layer with 3 neurons corresponds to the three behavior classes (Walking, Eating, Resting), with each neuron providing the predicted probability for each class.
        wandb: In machine learning, a fully connected layer connects every input feature to every neuron in that layer. A dense layer is mostly used as the penultimate layer after a feature extraction block (convolution, encoder or decoder, etc.), output layer (final layer), and to project a vector of dimension d0 to a new dimension d1.

    filters=64:
        Explanation: The number of filters in a convolutional layer represents how many unique features or "views" of the input data the model tries to capture. The choice of 64 is based on typical practices in deep learning where powers of 2 are frequently used. This convention is both for memory optimization and as a general heuristic. Depending on the complexity of your data, this number can be adjusted: too many might overfit, and too few might underfit.

    kernel_size=3:
        Explanation: The kernel size is the size of the "window" that slides over the input data, determining the number of consecutive data points the model looks at concurrently. For 1D sequences, a kernel_size of 2 or 3 is common since the goal is often to identify patterns between adjacent data points.

    activation='relu':
        Explanation: Activation functions introduce non-linearity into the model, enabling it to capture more complex relationships in the data. The Rectified Linear Unit (ReLU) function has proven to be effective in various neural network applications due to its simplicity and efficiency.

    padding='same':
        Explanation: Padding ensures that the output of a convolutional layer retains the same spatial dimensions as its input. Using "same" padding is beneficial because it ensures that the size doesn’t reduce too quickly as you add more convolutional layers.

    pool_size=2:
        Explanation: Pooling layers serve to reduce the spatial dimensions of the input. A pool_size of 2 is commonly used to halve the spatial dimensions of the data, aligning with the idea of capturing broader features as you go deeper into the network.






Things about base architecture: 
    -   The number of filters and the kernel size in the convolutional layers can be adjusted based on performance and experimentation.
    You might want to consider using Batch Normalization or Dropout layers to improve generalization.
    The architecture I've provided is a basic starting point; feel free to refine it as needed. Adjust the number of neurons in the dense layer, add more dense layers, or tweak other hyperparameters to suit your specific problem.
    Remember to preprocess your data correctly. Normalizing or standardizing input data can be crucial for training deep learning models effectively.
    - Flatten layer is only needed when not using the Transformer. 






# Notes on not using a Flatten layer



- We will use a stride-1 convolution to be able to add layers without changing the output size. 
- Our image will be size `h` by `w`, using a padding of `pad` and a stride of `stride`. Note that: The general formula for each dimension is (n + 2*pad - ks)//stride + 1, where pad is the padding, ks, the size of our kernel, and stride is the stride.
- 







# Notes on the flatten layer and the Transformer
Yes, the decision to avoid using a Flatten layer makes sense when you consider connecting the CNN to a Transformer. Here's why:

    Temporal Sequence Preservation: Transformers inherently process sequences. If you were to flatten the output of the CNN, you'd lose the sequential aspect of your data. The Transformer's architecture is designed to handle sequence data, with mechanisms like self-attention that allow each element in the sequence to consider other elements in the sequence. Flattening the data would remove the time dimension, rendering the Transformer unable to utilize its sequential processing capabilities.

    Positional Encoding: Transformers don't have a built-in sense of position or order for sequences since they treat input positions in parallel. To understand the sequence's order, positional encodings are added to the embeddings at the input layer. By preserving the sequential structure (and not flattening), you can efficiently add these positional encodings to the Transformer's input.

    Spatial Resolution: If the idea is to maintain some form of spatial resolution (even after all the convolution and pooling operations), you'd want to retain the 2D structure coming out of the CNN. Each position in the spatial dimension can then be treated as a 'token' or 'word' in the Transformer.

    Richer Representations: Retaining the 2D structure can allow for richer representations. Each position can have a multi-dimensional vector that carries more information compared to a flattened version. This is especially beneficial if the next steps involve complex operations like attention mechanisms.

    Integration with Transformer: If you connect the CNN to a Transformer, the output from the CNN would typically be reshaped to a sequence of vectors, where each vector represents a patch or position from the CNN's output. Each of these vectors would then be treated as an input 'token' to the Transformer.

In summary, by not flattening the CNN output, you're preserving the time-dependent structure of the data, which is crucial for the subsequent Transformer layers. It allows the model to capture both spatial (from the CNN) and temporal (from the Transformer) dependencies in the data.







# Notes on the CNN dimensions: 
- we start with 8 features but after the first 1DConv we end up with 64 "features", which are not features as such but channels, 
and we end up with 64 channels because we defined all the CNN layers to have 64 filters. 




# Notes on CNN + Transformers. 
I apologize for the confusion. The "64 features" I referred to comes from the number of filters in the Conv1D layers of the CNN, not the original input features. Each filter in a Conv1D layer is capable of detecting a specific feature in the input data. Since there are 64 filters, the output of the Conv1D layers will have 64 channels, each corresponding to the features detected by each filter.

Here’s a breakdown:

    Initial Input: Your initial data has 8 features (3 from the accelerometer, 3 from the magnetometer, pitch, and roll). Each feature is like a "channel" of data. For image data, the analogy would be RGB channels.

    After Conv1D Layers: After passing through a Conv1D layer with 64 filters, the data is transformed into 64 different feature maps. Each feature map is the result of convolving one of the 64 filters with the input data. So, even though you start with 8 features, the Conv1D layer transforms these into 64 derived features (feature maps).

    Transformer Input: When connecting to the Transformer, we’re feeding it these derived features, not the original 8 features. The Transformer processes these derived feature maps to capture temporal dependencies among them.

I hope this clarifies the jump from 8 to 64 features. The 64 comes from the number of convolutional filters, each detecting a different feature, transforming the original 8 input features into 64 derived feature maps. If you still have questions or need further clarification, feel free to ask!









We will feed the CNN 128second segments. These segments will contain 20 behaviour labels, each corresponding to 6.4 seconds of data. 
And since we have 40Hz data, each label will correspond to 40x6.4=256 elements in an array. And the whole segment will have 
40x6.4x20=5120 elements in an array. These 128 second segments will have 8 channels (one per each feature). 
Thus 1 input is a 5120x8 array. These 5120x8 arrays will pass through many 1D/2D Convolution + Max Pooling layers until 
we get to an output that is an array of 20x8, that is, until we have / compressed / downsampled / dimensionality reduced / 
the information stored in 1 behaviour label from 256 elements in the input array to 1 element in the output array. 
The dimensionality reduction will be done by the Max Pooling layer with a stride of 2, thus each MaxPool layer
will reduce in half the array? Colin said that each maxpool halves the input, thus 256 -> 128 -> 64 -> 32 -> 16 -> 8 -> 4 -> 2 -> 1 
but what about the other dimension, the one related to the 8 channels?

https://chat.openai.com/share/e735705c-6d05-4645-89f8-9a04c6e9b94a



Features included will be (Acc = Accelerometer, Mag = Magnetometer): 
Acc axis x, 
Acc axis y, 
Acc axis z, 
Mag axis x, 
Mag axis y, 
Mag axis z, 
Pitch angle, 
Roll angle



Accelerometer: Measures the acceleration forces acting on the sensor. These forces can be due to gravity 
(when the sensor is stationary) or due to motion. 
Thus, it can detect the orientation with respect to gravity (for static measurements) and sense movement.

Magnetometer: Measures the strength and direction of the magnetic field around the sensor. 
It's primarily used to determine the device's orientation relative to Earth's magnetic field, allowing 
it to determine magnetic north.

Pitch and Roll:

    Pitch (θ): This is the angle between the horizontal plane and the front-to-back axis (typically 
    denoted as the X-axis). When you tilt your head up and down, you're changing the pitch. It's 
    essentially the "nodding" motion.

    Roll (φ): This is the angle between the horizontal plane and the side-to-side axis (typically 
    denoted as the Y-axis). Imagine tilting your head from one shoulder to the other; this 
    action is changing the roll. It's like the "tilting" motion of your head.

Using the accelerometer data:

Pitch can be computed from the accelerometer readings using the following formula:

$$
\theta = arctan(frac)
$$


θ=arctan⁡(axay2+az2)θ=arctan(ay2​+az2​

​ax​​)
Where:

axax​, ayay​, and azaz​ are the accelerometer readings in the x, y, and z directions respectively.

Roll can be computed using:
ϕ=arctan⁡(ayax2+az2)ϕ=arctan(ax2​+az2​

​ay​​)

Note: The above calculations assume that the only acceleration the accelerometer senses is the acceleration due to gravity (i.e., the sheep is not in motion). If the sheep is moving, the readings will include both the acceleration due to motion and gravity, which complicates the interpretation.




Taking into account feature engineering (creating new transformations of the input data in order
to make it easier for the model), what about feature engineering in vision models?




A convolution applies a `kernel` across an image. A kernel is matrix (typically 3x3), matrices in the context of deep learning are 
functions. 


Convolutions extract information from the image?
Convolutions compress information from the image?

If you do a 2D convolution, the kernel is a matrix; if you do a 1D convolution, the kernel is a vector. 


when we do a convolution (i.e., we apply a kernel to an array and get another array as a result), we lose the surrounding pixels
of the image. Thus a convolution implies a loss in information. To avoid that we need to do 



Pytorch represents an image as a rank-3 tensor with dimensions `[channels, rows, columns]` (what about Tensorflow?)


Kernels in PyTorch need to be rank-4 tensors: 
`[channels_in, features_out, rows, columns]`


In later layers the convolutional kernels become complex transformations of features from lower 
levels, and these are complex transformations that we don't know how to manually construct ourselves.
Thus the best approach is to learn the values of the kernel (with SGD), that is, the model will learn
the features that are useful for classification. 


In order to do classification with CNNs, we need the output activation per image be a single element. 
One way to deal with this is to use enough stride-2 convolutions such that the final layer is size 1. 
That is, if the image is 28x28 then after one stride-2 convolution the size will be 14×14, after two 
it will be 7×7, then 4×4, 2×2, and finally size 1.




A `convolutional nerual network` it's just when we use convolutional layers insteas of (or in addition to) 
regular linear layers. 



# Strides and Padding
With appropriate padding, we can ensure that the output activation map is the same size as the original image, 
which can make things a lot simpler when we construct our architectures. <<pad_conv>> shows how adding padding 
allows us to apply the kernels in the image corners.

If we add a kernel of size ks by ks (with ks an odd number), the necessary padding on each side to keep the 
same shape is ks//2. An even number for ks would require a different amount of padding on the top/bottom 
and left/right, but in practice we almost never use an even filter size.

So far, when we have applied the kernel to the grid, we have moved it one pixel over at a time. But we 
can jump further; for instance, we could move over two pixels after each kernel application, as 
in <<three_by_five_conv>>. This is known as a stride-2 convolution. The most common kernel size in 
practice is 3×3, and the most common padding is 1. As you'll see, stride-2 convolutions are useful 
for decreasing the size of our outputs, and stride-1 convolutions are useful for adding layers 
without changing the output size.



In an image of size h by w, using a padding of 1 and a stride of 2 will give us a result of size (h+1)//2 by (w+1)//2. 
The general formula for each dimension is (n + 2*pad - ks)//stride + 1, where pad is the padding, ks, the size of 
our kernel, and stride is the stride.




Notes: 
- Using less convolutional layers will help reduce overfitting. i.e., we need N amount of 
Max Pooling layers to get to compress the 256 (6 dimensional) elements that represent the 6.4s block that 
have a single label into a 1 (6 dimensional) element that will be fed to the Transformer. But the amount of convolutional
layers can vary. 








# Theory: 
Max pooling is a common operation in the architecture of Convolutional Neural Networks (CNNs). It's used primarily for two main purposes:

    Downsampling (Dimensionality Reduction): As a CNN processes an image, it often produces feature maps that are spatially large. These large feature maps can be computationally expensive to process in deeper layers, and they can overfit to fine-grained details that may not be generally useful. Max pooling helps reduce the spatial dimensions (width and height) of these feature maps, which decreases the amount of parameters and computations in the network.

    Invariance to Small Translations: By selecting the maximum value from a local patch of values in the feature map, the output becomes somewhat invariant to small translations of the input. This means that if an object or feature in the image moves slightly, the max pooling operation can still detect and represent it.

Here's how max pooling works:

    Sliding Window: A window (often of size 2x2 or 3x3) slides over the input feature map.

    Select Maximum: For each position of the window, the maximum value within that window is taken.

    Stride: The window moves according to a specified stride. If the stride is 2 (which is common), then the window moves two units at a time, reducing the spatial dimensions of the feature map by half in each dimension. For instance, a stride of 2 on a 2x2 window would reduce a 4x4 feature map to a 2x2 feature map.

Visually, for a 2x2 window:

```console
Original 4x4 feature map:
a b c d
e f g h
i j k l
m n o p

After 2x2 max pooling with stride of 2:
f g
n o
```'''


Some things to note:

    No Parameters: Unlike convolutional layers, max pooling operations don’t have any trainable parameters. They simply perform an operation based on a defined window size and stride.

    Other Pooling Methods: Max pooling is not the only pooling method. Average pooling, which takes the average of the values within the window, is another method, though it’s less common than max pooling in CNNs for visual tasks.

    Potential Information Loss: One trade-off with max pooling is that by downsampling, you're discarding a lot of information. Modern architectures sometimes use other methods or combine pooling with other strategies to preserve more spatial information.

Despite its simplicity, max pooling has proven to be effective in many successful CNN architectures, such as AlexNet, VGG, etc. However, newer architectures like ResNet and DenseNet have shown that it's possible to achieve great performance without aggressive downsampling in the early layers.


Other methods and strategies to Max Pooling to avoid Potential Information Loss: 
    Strided Convolution: Instead of using pooling layers for downsampling, some architectures use convolutional layers with a stride greater than 1. This way, you still get a downsampling effect, but unlike pooling, the operation involves learnable parameters, which might capture more meaningful information during downsampling.

    Average Pooling: As opposed to max pooling, which takes the maximum value in a window, average pooling takes the average. This can sometimes preserve more general information than max pooling, though it's less commonly used for downsampling in modern architectures.

    Global Average Pooling (GAP): Towards the end of a CNN, instead of using fully connected layers, some architectures use GAP. This operation takes the average of each feature map and produces a single value for each. It helps in reducing the total number of parameters, especially in the last layers, and can sometimes lead to better generalization.

    Dilated Convolution: Dilated convolutions, also known as atrous convolutions, introduce gaps in the kernel application. By using dilated convolutions, one can increase the receptive field of neurons without increasing the number of parameters or the computational cost.

    Depthwise Separable Convolutions: Popularized by architectures like MobileNet, these are a factorized version of convolutions that split the convolution operation into depthwise convolution followed by pointwise convolution. This allows for reduced computational costs while still capturing spatial and depth information effectively.

    Skip Connections and Residual Blocks: Used prominently in architectures like ResNet, skip connections allow for the direct transfer of information from earlier layers to later layers. This helps in retaining spatial information and also addresses the vanishing gradient problem in deep networks.

    U-Net Architecture: Particularly popular in segmentation tasks, the U-Net architecture introduces an upsampling path that complements the downsampling path of the network. This helps in retaining and recovering spatial information for precise localization in tasks like image segmentation.

    Pyramid Pooling: This involves pooling operations at different scales and then concatenating them. It's a way to capture both local and global context in an image.

    Spatial Pyramid Pooling (SPP): A layer that converts the arbitrary size of feature maps into a fixed-size output. It divides the feature map into different scales and performs pooling at each scale, making the network robust to various input sizes.

    Attention Mechanisms: Borrowed from the realm of NLP and transformer architectures, attention mechanisms can weigh the importance of different spatial locations differently, focusing the network's "attention" on more crucial areas of an image.

These are just some of the prominent methods and strategies that have emerged as alternatives or enhancements to traditional pooling in CNN architectures. The best approach often depends on the specific problem being addressed and the constraints of the computational environment.




# convTransformer layers explanation

CNN Part:
Conv1D:

Explanation: Conv1D stands for one-dimensional convolution layer. It is highly effective for dealing with sequential or temporal data. Each filter in a Conv1D layer slides across the sequence and computes dot products between its weights and the local region in the input sequence, which allows the model to recognize local patterns and features. This characteristic makes Conv1D suitable for tasks like yours, where the goal is to extract meaningful features from sequences of animal movement data. The selection of 64 filters enables the model to detect a diverse set of patterns, enhancing its learning capability and representational power.
MaxPooling1D:

Explanation: MaxPooling1D layers systematically reduce the spatial (or in the case of sequences, temporal) dimensions of the input volume, leading to both computational efficiency and a reduction in the likelihood of overfitting. By selecting the most prominent features (maximum values) within specified windows and reducing dimensionality, the model becomes somewhat invariant to translation, and gains computational benefits from the reduced volume of data. The repeated use of max-pooling ensures the gradual focusing on more abstract features as the data proceeds through the network.
Reshape:

Explanation: The reshape layer is instrumental when transitioning from CNN to Transformer layers. The purpose is to align the data format with the input requirements of the Transformer. In this context, it’s converting the output of the CNN layers into a sequence of vectors, each with a length of 64, to prepare a suitable input for the Transformer, effectively transforming the spatial features into a format interpretable as a sequence.

Transformer Part:
MultiHeadAttention:

Explanation: The MultiHeadAttention layer allows the model to focus on different parts of the sequence simultaneously. This concurrent attention mechanism enables the capturing of various types of features and dependencies in the data, enhancing the model's expressiveness and learning capacity. In practice, multiple attention heads provide diverse perspectives on the input data, leading to more comprehensive feature representation and improved performance in tasks involving complex, sequential patterns.
Add and LayerNormalization:

Explanation: These layers are part of the residual connection and layer normalization steps in the Transformer architecture. The Add layer adds the input sequence to the output of the MultiHeadAttention layer, forming a residual connection that helps in training deeper models by mitigating issues like vanishing gradients. LayerNormalization then normalizes the data to have a mean of 0 and a standard deviation of 1, which stabilizes the training process and helps in faster convergence.
Position-wise Feed-Forward Networks:

Explanation: These networks consist of two Conv1D layers used for feature transformation. They follow the self-attention mechanism in each Transformer block and provide another level of transformation of the learnt features. In practice, these networks add to the model’s capacity without introducing additional connections between the sequence elements, keeping the computational complexity manageable.
GlobalAveragePooling1D:

Explanation: This layer is used to reduce the dimensionality of the temporal sequence while retaining the essential features. It computes the average value for each feature over all the time steps, resulting in a fixed-length output vector. This summarization of the sequence data into a compact representation enables connection to subsequent dense layers for classification without an explosion in parameters or computational demand.

Final Dense Layers:
Dense:

Explanation: The Dense layers at the end serve the role of classifiers. After feature extraction and transformation by the CNN and Transformer, the dense layers map the learnt features to the specific classes of behaviors. The use of ‘relu’ activation function adds non-linearity, enhancing the model's capacity to learn complex mappings. The final dense layer with 3 units corresponds to the 3 classes of behaviors, and softmax activation function is used to convert the logits into probabilities, making the output interpretable as the likelihood of each behavior class.
Overall Architecture:

Explanation: This CNN + Transformer model is designed to exploit both the spatial and temporal features in your animal movement data effectively. The CNN part, with its Conv1D and MaxPooling1D layers, is tailored to extract spatial features and patterns from the high-dimensional raw data efficiently. The Transformer part then takes these extracted features and captures the temporal dependencies amongst them, offering a rich, sequence-aware feature set. This combination aims to provide a comprehensive understanding of both the intricate patterns in individual data segments (thanks to the CNN) and the broader temporal dynamics and dependencies (thanks to the Transformer), leading to a well-rounded, robust model for behavior classification.




# Number of Heads in the MultiHeadAttention layers: 
In a Transformer model, the MultiHeadAttention layer is essential for the model to focus on different parts of the input sequence when producing an output sequence. Each "head" in the MultiHeadAttention layer can be thought of as a separate attention mechanism, allowing the model to focus on different aspects or features in the data.

Here’s a breakdown of the role of the number of heads and why you might choose to use more or fewer heads:
1. Parallel Attention Mechanisms:

    More Heads: The model can pay attention to multiple parts or aspects of the input simultaneously. It can capture a richer set of features and relationships in the data.
    Fewer Heads: The model has fewer attention mechanisms, potentially making it less able to capture a diverse set of features and relationships.

2. Model Capacity:

    More Heads: Increases the capacity of the model, potentially allowing it to learn more complex patterns and representations.
    Fewer Heads: Reduces the model’s capacity, which can sometimes be beneficial to prevent overfitting on smaller datasets.

3. Computational Resources:

    More Heads: Increases the computational requirements (both memory and compute) for training and inference.
    Fewer Heads: Makes the model more computationally efficient but at the risk of reducing its ability to capture complex patterns.

4. Interpretable Attention Patterns:

    More Heads: Can lead to more interpretable attention patterns, where different heads focus on different types of relationships or features.
    Fewer Heads: Can make the attention patterns less diverse.

Guidelines on Selecting Number of Heads:

    Empirical Testing: It's often empirical; testing different numbers of heads can give insights into what works best for a specific task or dataset.

    Divisibility Constraint: The number of heads should be a factor of the model’s embedding dimension size. It ensures that the division of dimensions among heads is clean and does not require additional handling.

    Trade-Offs: Consider the trade-offs between computational efficiency and model performance. More heads might lead to better performance but will require more computational resources.

Conclusion:

Choosing the right number of heads is task-dependent and requires experimentation. While having more heads can potentially lead to improved performance by capturing a wider range of patterns and dependencies in the data, it also increases the computational requirements. Balancing these factors is key to optimizing the performance of Transformer models for specific tasks and computational budgets.






# Things to take into account for the publication. 
Your model looks structurally sound for the most part, but there are always aspects that can be scrutinized, especially in the context of preparing for a scientific publication. Here's a detailed review:
Structure and Components

    Conv1D Layers:
        The use of Conv1D layers for sequence data, especially for time series or sensor data, is a good choice. It captures local patterns effectively. Ensure that the kernel size and the number of filters are optimized; they should be determined empirically or justified theoretically.

    MaxPooling1D Layers:
        Max-pooling is used extensively to downsample the feature maps and is quite standard. However, in the context of a scientific publication, you need to explain why max-pooling is suitable for your data and task, and ensure to explore or discuss alternative pooling methods (average pooling, global pooling, etc.).

    Transformer Layers:
        The Transformer’s architecture makes sense, and it's an advanced way of capturing temporal dependencies. However, you may want to consider the sequence length; in some cases, a length of 20 might be short for capturing complex temporal patterns. The number of heads and key dimensions in the MultiHeadAttention layer should also be validated empirically.

    Feedforward and Pooling Layers:
        The feedforward and pooling layers following the Transformer are standard, but the architecture’s depth and complexity need to be justified, possibly with ablation studies or comparisons to simpler models.

Hyperparameters

Hyperparameters, like the number of filters, kernel sizes, and the architecture of the feedforward network after the Transformer, should be either empirically optimized or theoretically justified. Cross-validation, grid search, or Bayesian optimization methods can help in tuning these hyperparameters.
General Points

    Empirical Validation:
        Every choice in the model should ideally be backed by empirical results. For example, is the complexity of the model justified given the amount and complexity of your data? How does the model perform compared to simpler baselines or more complex alternatives?

    Interpretability:
        For a scientific publication, interpretability can be crucial. Consider adding components or analyses that can help in interpreting the model's decisions. This can involve attention visualization, feature importance analysis, or other interpretability techniques.

    Reproducibility:
        Ensure that every aspect of the model and preprocessing is documented so that the experiments can be reproduced. This involves fixing random seeds, documenting the exact versions of libraries, and detailing the training procedure.

Adjustments and Checks

    Ensure that the CNN and Transformer parts are not conflicting. They should complement each other. Empirically verify that both parts contribute to the performance improvement.
    The Transformer layer's hyperparameters (e.g., the number of attention heads, key dimensions) should be chosen carefully and tested for effectiveness.
    Consider conducting ablation studies to demonstrate the contribution of each component of your model.

Code Review

Your code looks clean and should work as expected with TensorFlow/Keras. Make sure to train the model with a suitable optimizer, loss function, and metrics to evaluate its performance effectively. Also, consider adding regularizations if necessary to prevent overfitting, especially given the model's complexity.

In conclusion, the model is a good starting point, but each choice and component should be validated, justified, and potentially compared to alternatives to make the work publication-ready. Always consider the reviewers’ perspective: they will ask “why was this specific choice made?” and “how does this component contribute to the overall performance?”. Being able to answer these questions, either with empirical results or theoretical justifications, is crucial for a scientific publication.






# Why two subsequent conv1d layers in transformer part

The two consecutive Conv1D layers are part of the position-wise feed-forward network in the Transformer. The original Transformer model, as proposed by Vaswani et al. in their 2017 paper "Attention is All You Need," includes a position-wise feed-forward network consisting of two fully connected layers with a ReLU activation in between. In the implementation you've shown, this structure is replicated using 1D convolutional layers with kernel size 1, which is equivalent to fully connected layers when applied to each position separately.

Here is the breakdown:

    First Conv1D Layer:
        Explanation: The first Conv1D layer with 128 filters and a kernel size of 1 is used to increase the dimensionality of the input data. This is known as a bottleneck layer. In practice, this helps the network learn a richer set of features. Increasing the dimensionality can enhance the expressiveness of the layer and its ability to capture complex features. The use of the ReLU activation function adds non-linearity to the model, enabling it to learn and represent more complex patterns and dependencies.

    Second Conv1D Layer:
        Explanation: The second Conv1D layer reduces the dimensionality back to 64. This dimensional reduction aims to compress the learnt features into a more compact representation, enhancing the model's efficiency. It plays a similar role to the second fully connected layer in the original position-wise feed-forward network of the Transformer. The combination of expanding and then compressing the dimensionality allows the model to learn a diverse range of features while maintaining a manageable computational load.

These two layers essentially form a bottleneck architecture, a concept often used in deep learning to balance model expressiveness and computational efficiency. The increasing and decreasing of dimensions can capture a wider variety of features in the data while also maintaining a compact and efficient model structure. In the context of a scientific publication, it would be beneficial to justify these choices, either theoretically or empirically, to demonstrate their effectiveness for the specific task and data at hand.



# Why 2 Dense layers instead of 1 at the end of the ConvTransformer:

The use of two dense layers in sequence, rather than a single layer, is a design choice meant to increase the model's capacity to learn complex mappings from its inputs to outputs. It can aid in capturing more intricate patterns and representations in the data. Here’s a breakdown:
First Dense Layer:

    Explanation: The first dense layer with 20 units and ReLU activation serves as a hidden layer that captures complex relationships in the data. ReLU is chosen for its efficiency and capability to introduce non-linearity into the model, allowing it to learn and represent more complex patterns. Having this intermediary layer increases the model’s expressiveness. It acts as a feature transformer, converting the high-level features extracted and processed by the preceding CNN and Transformer layers into a representation that can be effectively used for the final classification.

Second Dense Layer:

    Explanation: The second dense layer serves as the output layer of the model, consisting of 3 units, each corresponding to one of the classes (Walking, Eating, Resting). The softmax activation function converts the raw scores (logits) into probabilities, ensuring that they are non-negative and sum up to one. Each unit’s output can be interpreted as the model's confidence that the input data belongs to one of the three behavior classes.

Theoretical & Practical Justification:

    The addition of more than one dense layer allows the network to learn hierarchical features. The first dense layer can learn combinations of the high-level features extracted by the CNN and Transformer layers. The subsequent dense layer can then learn combinations of those combinations, adding layers of abstraction and complexity to the feature learning process.
    In practice, adding an extra dense layer can lead to a more nuanced and accurate classification, especially for complex tasks. However, it also introduces additional parameters and complexity to the model, which necessitates careful consideration to avoid overfitting. Regularization techniques, dropout, or other methods might be needed to manage this increased complexity.

In a scientific context, each layer and the number of units should be justified either through empirical results (e.g., cross-validation performance) or theoretical reasoning based on the nature of the data and the task. Also, an ablation study can be a powerful tool to demonstrate the contribution of each layer to the model's overall performance.






# Reducing overfitting
- Label smoothing: check this paper check Obsidian's `dl_label-smoothing` or https://arxiv.org/abs/1512.00567












Abstract: 



last draft: 
Las redes neuronales son revolucionarias, hoy en día esto es evidente.
Como lo demostró el Teorema Universal de Aproximación, una red neuronal con suficiente cómputo puede aproximar cualquier función, y en este mundo TODA acción, fenómeno o conocimiento se puede representar con una función. Eso quiere decir que las redes neuronales pueden potencialmente hacerlo todo.
Hace más de 30 años, esta realización llevó a los primeros partidarios de las redes neuronales -hoy los padres de la Inteligencia Artificial- a apostarlo todo por lo que hoy se conoce como el deep learning. Esta apuesta dio lugar a los Generative Pre-trained Transformers, o modelos GPT, que hoy lideran la revolución de la IA. ChatGPT se lo demostró a todo el mundo y sentó un antes y un después en la historia de la IA. Esto sin duda es una revolución, pero una que recién empieza. Ahora el conocimiento de todo el mundo está al alcance de la mano de cualquiera, a solo una pregunta de averiguarlo todo. Este parece un momento pivotal para la humanidad, comparable con el invento de la imprenta que permitió que los libros -y por lo tanto el conocimiento- estén al alcance de todos.
Pero por el momento la mejor tecnología GPT es de código cerrado, bajo el argumento de que esta tecnología es muy peligrosa para que el ciudadano común acceda a ella. Es este argumento nuevo? acaso no se argumentó lo mismo en el siglo 15 con la imprenta y los libros de acceso masivo? qué hubiera pasado si un monopolio mundial se hubiera quedado con esta antigua tecnología? qué hace que una tecnología sea benéfica para toda la sociedad y cuál es el rol de la comunidad de código abierto en todo esto?
Esta es una charla sobre cómo un grupo de amigos desarrolladores vieron todo lo que estaba pasando, decidieron meterse y crearon a ALI, el primer Asistente Legal Inteligente que busca democratizar el acceso al conocimiento legal.
Pero sobre todo, esta charla es sobre cómo podés involucrarte con la IA y crear tu propio sistema inteligente.


old draft: 
Las redes neuronales son revolucionarias, hoy en día esto es evidente. 
Como lo demostró el Teorema Universal de Aproximación, una red neuronal con suficiente cómputo puede
aproximar cualquier función, y en este mundo TODA acción, fenómeno o conocimiento se puede 
representar con una función. Eso quiere decir que las redes neuronales pueden potencialmente hacerlo todo.
Hace más de 30 años, esta realización llevó a los primeros partidarios de las redes neuronales -hoy los padres
de la Inteligencia Artificial- a apostarlo todo por lo que hoy se conoce como el deep learning, apuesta que 
dio lugar a los Generative Pre-trained Transformers, o modelos GPT, que hoy lideran la revolución de la IA. 
ChatGPT se lo demostró a todo el mundo y sentó un antes y un después en la historia de la IA. Esto sin duda
es una revolución, pero una que recién empieza. Ahora el conocimiento de todo el mundo
está al alcance de la mano de cualquiera, a solo una pregunta de averiguarlo todo. Este parece un momento pivotal
para la humanidad, comparable con el invento de la imprenta que permitió que los libros -y por lo tanto el conocimiento- estén al alcance de todos. 
Pero por el momento la mejor tecnología GPT es de código cerrado, bajo el argumento de que esta tecnología es muy 
peligrosa para que el ciudadano común acceda a ella. Es este argumento nuevo? acaso no se argumentó lo mismo
en el siglo 15 con la imprenta y los libros de acceso masivo? qué hubiera pasado si un monopolio mundial se hubiera
quedado con esta antigua tecnología? qué hace que una tecnología sea benéfica para toda la sociedad y cuál es el rol de 
la comunidad de código abierto en todo esto?
Esta es una charla sobre cómo un grupo de amigos desarrolladores vieron todo lo que estaba pasando, decidieron meterse y 
crearon a ALI, el primer Asistente Legal Inteligente basado en modelos GPT que busca democratizar el acceso al conocimiento legal. 
Pero sobre todo, esta charla es sobre cómo podés involucrarte con la IA y crear tu propio sistema inteligente. 










qué hace que una tecnología impacte benéficamente en la sociedad?
qué hace que una revolución tecnológica termine en buen puerto? 

Esta charla explora el advenimiento de la IA de uso masivo, cómo un grupo de amigos desarrolladores se 
reunieron para desarrollar a ALI -el primer Asistente Legal Inteligente-, y lo más importante de todo, 
cómo el trabajo de la comunidad de código abierto da lugar a que cualquiera haga lo mismo



Esta charla explora el advenimiento de la inteligencia artificial de uso masivo, el rol de la comunidad de 
código abierto en esta revolución, cómo un grupo de amigos desarrolladores vieron todo el potencial  y se





Como nos conocimos, por que consideramos que esto era importante y que nos teniamos que meter a fondo en esto.
Teoría introductoria, teorema de aproximación de funciones, emergencia del razonamiento, historia del AI y del deep learning.
Revolución del 2023, antes y despues de chatGPT
Nueva frontera de problemas ahora resolubles, problema del acceso al conocimiento (proyecto alexandria) .
Cómo construir sobre el gran cerebro llamado GPT y Cómo podés encarar uno de estos problemas vos mismo, relatos del desarrollo de ALI.
Cuestiones a notar: GPT no implica OpenAI, importancia del trabajo colectivo open source. Hitos alcanzados por la comunidad (huggingface, tinycorp). Por qué hacer la IA open source es tan importante.





Por qué las redes neuronales son revolucionarias?
Todo se apoya en el Teorema Universal de Aproximación, que prueba que si 
tenemos una red neuronal con ciertas características y con suficientes datos, 
aproximar cualquier función. Y? por qué es importante esto? qué vieron los partidarios del 
deep learning hace más de 30 años? La cuestión reside en que TODO lo que ocurre en el mundo
se puede representar como una función. Gran parte de la ciencia es encontrar la función que describe un
fenómeno determinado. Pero estamos limitados a tener que describir explícitamente esa función. 
Y eso es una enorme limitación. Cuál es la función que describe la traducción de un lenguaje a otro?
cuál es la función para manejar un auto? cuál es la función que convierte señales cerebrales a inputs
en una computadora? No tenemos idea y capaz que nunca podamos describir esas funciones en papel. 
Pero derrepente, según prueba el Teorema Universal de Aproximación, tenemos un aparato matemático y
computacional que puede encontrar estas funciones. Eso quiere decir que potencialmente, las redes 
neuronales pueden resolver absolutamente todo. Claro que esto hoy en día no es así y encontrar una 
función relativamente simple requiere un esfuerzo enorme de ingeniería, recolección de datos y poder 
de cómputo. Las primeras redes neuronales que empezaron a resolver problemas grandes arrancaron hace
aproximadamente 10 años, con lo que se conoce como aprendizaje supervisado. Agarramos los datos de
input (e.g., imágenes), los datos de output (e.g., texto que describa el contenido de la imagen) y 
lanzamos un proceso de optimización para encontrar la función que mapea los inputs con los outputs. 
Esto funciona pero tiene grandes problemas, como la necesidad de tener un montón de datos, porque estas redes
con este enfoque de aprendizaje no puede aprender nada que no vieron. 
Esto en parte se empieza a resolver con algo que se llama aprendizaje auto-supervisado, o dicho / en criollo / sencillamente /, 
/"completame lo que falta" / "fill in the gap"/. Esta estrategia es muy simple y resuelve en gran parte la necesidad de tener datos
anotados. La cuestión es que completar lo que falta, tarea que parece facil, requiere de entender como funcionan las cosas. 
Porque por ejemplo, completar la frase / "the cat is under the __" / "me voy a ir a la cama a ___" (dormir) / no parece tan dificil. 
Pero que pasa si lo que hay que completar es un problema matematico, como por ejemplo "\sqrt[3]{27} = ___". Derrepente esto 
no es tan fácil. Esto requiere que la red neuronal cree una especie de "modelo mental" de cómo funciona la matemática. 
Esto lo logra la red neuronal durante el entrenamiento, es como un objeto matemático y computacional que muta y se modifica para poder
encontrar la funcion que describa este aparente razonamiento. Primer señal de que hay algo emergente ahí, acaso empezó a emerger el razonamiento?
Fast forward 30 de noviembre 2022. Sale ChatGPT. El modelo de OpenAI entrenado usando aprendizaje auto-supervisado y una cantidad de 
computo masiva (bitter lesson de Rich Sutton). 

START Tangente:
Gran parte de la historia del AI se basó en investigadores y desarolladores que querían enseñarle a la máquina, inyectarles el conocimiento. 
(tal vez eso dice bastante de nosotos). La premisa era esa, todo lo que queramos que sepa, se lo tenemos que dar nosotros. Queremos que la 
máquina entienda lógica? le tenemos que meter un módulo de lógica. Queremos que entienda el lenguaje? tenemos que meterle las reglas semánticas de 
alguna manera. Durante varias décadas, este fue el approach dominante. Casi toda la investigación que se hacía era en cómo inyectarle el conocimiento 
que nosotros tenemos a las máquinas de una forma eficaz y eficiente. Pero allá por los 80s/90s algunos empezaron a pensar que no tenía mucho sentido
eso, que en verdad no escalaba mucho. Que no había forma de enseñarle todo el conocimiento del mundo a una máquina, y que lo mejor era que la máquina
lo aprenda por sí misma. Ahí es donde empieza a surgir esta nueva oleada de AI. AI que aprende sola (i.e., aprendizaje auto-supervisado). 
Rich Sutton dice: 
"
As in the games, researchers always tried to make systems that worked the way the researchers thought their own minds 
worked---they tried to put that knowledge in their systems---but it proved ultimately counterproductive, and a colossal 
waste of researcher's time, when, through Moore's law, massive computation became available and a means was found to put it to good use.
[...]
This is a big lesson. As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes. 
To see this, and to effectively resist it, we have to understand the appeal of these mistakes. We have to learn the bitter lesson
that building in how we think we think does not work in the long run. The bitter lesson is based on the historical observations 
that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is 
personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 
4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. 
The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, 
human-centric approach. 
[...]
The second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, 
irredeemably complex; we should stop trying to find simple ways to think about the contents of minds, such as simple 
ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, 
outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the 
meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good 
approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like 
we can, not which contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done.
"

OpenAI abraza esta premisa al máximo con sus modelos GPT: Transformers, aprendizaje auto-supervisado y una tonelada de computo. Y es esto
lo que sin duda rompe con todo lo que se había visto hasta ahora. 


END Tangente. 


La llegada de ChatGPT fue la primer señal grande al mundo de que la AI llegó y está para quedarse. 
Parece que con ChatGPT, OpenAI trajo una revolución. 

Pero es OpenAI el que trajo la revolución? o fueron los que dieron el primer vistaje de que esto era una revolución y en realidad la revolución
la inició Meta al liberar a Llama-2 y la revolución va a ser llevada a cabo por la comunidad open-source?
Qué es mejor? qué queremos como sociedad?

Pero OpenAI trajo la verdadera revolución? o fue Meta con Llama2 y la comunidad open-source?

Sin duda OpenAI 

asi como el conocimiento de cómo hacer ChatGPT está / capturado / enjaulado / atrapado / solo en las manos / por OpenAI, 
mucho del conocimiento del mundo está "atrapado" (no lo podemos obtener) o está detrás de un paywall "pagame y te explico". 
Esto no es algo nuevo, esto viene desde hace siglos. 




Pero qué queremos como sociedad? no nos olvidemos que la historia no terminó (lol a Francis Fukuyama) y que como sociedad
tenemos que tomar decisiones para orientar hacia dónde vamos, para determinar qué se va a escribir sobre el siglo 21. 

Conrad Gessner (1516-1565), a famous Swiss physician was against the PRINTING PRESS At the time, there were 10,000 book titles published by the printing press.
This shocked and worried Gessner as he believed that ordinary people could not handle so much knowledge. 
He demanded European countries to enforce a law that regulates the sale and distribution of books to ordinary people. 


Qué está pasando con esto de regular la IA? algo así ya pasó en la historia?
Si pensamos a la IA como una herramienta que brinda conocimiento a todas las personas, algo así ya ocurrió en el siglo 16 con la 
creación de la imprenta. Derrepente había una herramienta que podía traer de forma masiva conocimiento a todo individuo. 
TAL VEZ NO FRAMEARLO CON EL TEMA DE LA REGULACION PROQUE NO VAMOS A HABLAR MUCHO DE ESTO. 

Algo como lo que está por pasar ya pasó en la antiguedad?
Cuestión que allá por el siglo 15 un alemán llamado Johannes Gutenberg inventó la imprenta para el alfabeto latino y derrepente la cantidad de libros
en circulación aumentó exponencialmente. La gente tuvo mucho más acceso al conocimiento y que pim que pam, 
apareció la democracia, el racionalismo, el secularismo, la ciencia y la ilustración. 
Pero fue tan así la cosa? fue el invento de la imprenta lo que llevó a todos estos logros de la sociedad actual?
o fue la diseminación de esta tecnología revolucionaria lo que logró esto? qué hubiera pasado si nuestro Johannes
se hubiera armado un monopolio mundial de la imprenta, bajo la excusa de que era una tecnología demasiado peligrosa
para que esté en manos de todos?
Suena similar? cough cough OpenAI. 


Quien lidera la revolución? quien inventa la tecnología o quien la / divulga / hace accesible para todos /?



La prensa de imprimir asustó a muchas personas: la Iglesia católica, los gobernantes del Imperio otomano (que la prohibieron), e incluso este médico suizo.
Todos temían que los libros fueran demasiado peligrosos para poner en manos del público.
De alguna manera, todos tenían razón.
Los libros debilitaron sus poderes al hacer que la gente fuera más inteligente y conocedora.
El feudalismo, la autocracia, el pensamiento mágico y el dogma religioso rígido fueron eventualmente reemplazados por la democracia, el racionalismo, el secularismo, la ciencia y la Ilustración.
Y todos estamos mejor por ello.



George Hotz habla de esto también. Su premisa es la siguiente: Si a la IA pueden acceder solo un grupo reducido de personas, 
solo la gente "mala" va a acceder a ella. Esto se apoya en la idea de que si la IA está restringida, las únicas personas
que puedan acceder son las que tienen mucho poder (político, económico, etc) y, debido a los intereses e incentivos de
los sistemas políticos y económicos actualuales, las personas que tienen mucho poder (osea, las que van a acceder a la IA), 
son las más psicópatas y turbias, porque tuvieron que desarrollar esas características para llegar a donde están. Osea, 
algo como que la escalera que te lleva a lo más alto de la política y la economía es un cierto proceso de filtración, 
donde solo las personas dispuestas a bastantes cosas polémicas llegan, y que además de ser un proceso de filtración, 
es un proceso de retroalimentación para estos comportamientos psicópatas y turbios. Esta idea está muy bien desarrollada
por el politólogo Brian Klaas del Colegio Universitario de Londres. (Tiene una charla muy buenaque se llama "Why
psychopaths rise to power" y sino un libro que se llama "Corruptible: who gets power and how it changes us")







Por último, esta charla trata de nosotros, de cómo nos juntamos un día, reconocimo el potencial de esta nueva tecnología, 
y empezamos un ping pong de ideas sobre qué área podíamos impactar más con esta nueva tecnología. 


cómo un grupo de amigos, se juntó en la casa de uno de ellos a empezar a armar un proyecto que terminó en ALI. 


En esta charla, / los fundadores de legalia / X / exploran la nueva revolución de la IA, sus impactos en la sociedad (mmmm), 



Socrates feared writing … ----------------- CHECK ------------------------
« Your invention will enable them to hear many things without being properly taught, and they will 
imagine that they have come to know much while for the most part they will know nothing. »






History has not ended yet. We are still creating it. We better help to steer it to a good place. 





Y si nos convertimos en algo como el printing press shop de la IA?
After the invention of the printing press, its use started spreading across Europe. By the end of the 15th century, 
just 50 years after Gutenberg's invention, there were printing shops in more than 200 cities across Europe. 
These presses produced millions of books, which helped spread knowledge and ideas at an unprecedented rate. 
This proliferation of printed materials played a significant role in the rise of the Renaissance, the Reformation, and the Scientific Revolution.


Y si evaluamos las respuestas que damos en el momento de dar la respuesta? Algo como cuanta atención le prestó al contexto, o cuán 
relevante es el contexto a la pregunta. Este puntaje podría darlo un LLM y aparecer junto a la respuesta al usuario. 
Nos serviría para debuggear y nos serviría para informarle al usuario cuán seguros estamos de nuestra respuesta. 


Legalia, the new printing press shop of AI. 





Conocimiento al alcance de la mano: de la imprenta a los GPT





gotta see the broad picture, but at which scale? cuz the broad picture means seeing all the different parts at the same time
and seeing the relations between them. But of course the parts that you see depend on the scale you are looking at. 